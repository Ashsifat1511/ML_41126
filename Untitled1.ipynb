{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gene expression data...\n",
      "Expression data shape: (57736, 285)\n",
      "Metadata shape: (285, 46)\n",
      "Common samples: 285\n",
      "Final data shape: (285, 57736)\n",
      "Classes: ['Breast' 'CRC' 'GBM' 'HC' 'Hepatobiliary' 'Lung' 'Pancreas']\n",
      "Label distribution: [39 42 40 55 14 60 35]\n",
      "Performing feature selection using univariate method...\n",
      "Feature selection completed: (285, 57736) -> (285, 2000)\n",
      "Training multiple models...\n",
      "Training Random Forest...\n",
      "Random Forest CV Accuracy: 0.509 (+/- 0.070)\n",
      "Training SVM...\n",
      "SVM CV Accuracy: 0.465 (+/- 0.145)\n",
      "Training Logistic Regression...\n",
      "Logistic Regression CV Accuracy: 0.675 (+/- 0.184)\n",
      "Training Gradient Boosting...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Gene Expression Cancer Classification Pipeline\n",
    "Dataset: GSE68086 - Tumor-Educated Platelet (TEP) RNA sequencing data\n",
    "\n",
    "This pipeline processes gene expression data from tumor-educated platelets\n",
    "to classify different cancer types including Breast, Lung, GBM, CRC, Pancreas,\n",
    "Hepatobiliary cancers and Healthy Controls.\n",
    "\n",
    "Author: Generated for GSE68086 dataset analysis\n",
    "Date: September 2025\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CancerClassificationPipeline:\n",
    "    \"\"\"\n",
    "    A comprehensive machine learning pipeline for cancer classification\n",
    "    using gene expression data from tumor-educated platelets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, expression_file, metadata_file):\n",
    "        \"\"\"\n",
    "        Initialize the cancer classification pipeline\n",
    "\n",
    "        Args:\n",
    "            expression_file (str): Path to gene expression data CSV\n",
    "            metadata_file (str): Path to sample metadata CSV\n",
    "        \"\"\"\n",
    "        self.expression_file = expression_file\n",
    "        self.metadata_file = metadata_file\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.feature_names = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "        self.trained_models = {}\n",
    "\n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"Load and preprocess the gene expression data and labels\"\"\"\n",
    "        print(\"Loading gene expression data...\")\n",
    "\n",
    "        # Load expression data\n",
    "        expression_data = pd.read_csv(self.expression_file, index_col=0)\n",
    "        print(f\"Expression data shape: {expression_data.shape}\")\n",
    "\n",
    "        # Load metadata\n",
    "        metadata = pd.read_csv(self.metadata_file)\n",
    "        print(f\"Metadata shape: {metadata.shape}\")\n",
    "\n",
    "        # Extract labels from metadata\n",
    "        sample_labels = {}\n",
    "        for idx, row in metadata.iterrows():\n",
    "            sample_id = row['!Sample_source_name_ch1'].strip('\"')\n",
    "\n",
    "            # Extract cancer type from characteristics\n",
    "            cancer_type = None\n",
    "            characteristics = []\n",
    "            for col in metadata.columns:\n",
    "                if 'characteristics' in col and pd.notna(row[col]):\n",
    "                    char = row[col].strip('\"')\n",
    "                    characteristics.append(char)\n",
    "\n",
    "                    # Look for cancer type in characteristics\n",
    "                    if 'cancer type:' in char:\n",
    "                        cancer_type = char.split('cancer type:')[-1].strip()\n",
    "\n",
    "            # If no cancer type found in characteristics, infer from sample name\n",
    "            if cancer_type is None:\n",
    "                if 'HC' in sample_id or 'HD' in sample_id:\n",
    "                    cancer_type = 'HC'  # Healthy Control\n",
    "                elif 'Breast' in sample_id:\n",
    "                    cancer_type = 'Breast'\n",
    "                elif 'GBM' in sample_id:\n",
    "                    cancer_type = 'GBM'\n",
    "                elif 'Lung' in sample_id or 'NSCLC' in sample_id:\n",
    "                    cancer_type = 'Lung'\n",
    "                elif 'CRC' in sample_id:\n",
    "                    cancer_type = 'CRC'\n",
    "                elif 'Pancreas' in sample_id:\n",
    "                    cancer_type = 'Pancreas'\n",
    "                elif 'Hepatobiliary' in sample_id:\n",
    "                    cancer_type = 'Hepatobiliary'\n",
    "                else:\n",
    "                    cancer_type = 'Unknown'\n",
    "\n",
    "            sample_labels[sample_id] = cancer_type\n",
    "\n",
    "        # Align samples between expression data and labels\n",
    "        common_samples = list(set(expression_data.columns) & set(sample_labels.keys()))\n",
    "        print(f\"Common samples: {len(common_samples)}\")\n",
    "\n",
    "        # Filter data to common samples\n",
    "        self.X = expression_data[common_samples].T  # Transpose so samples are rows\n",
    "        self.y = [sample_labels[sample] for sample in common_samples]\n",
    "        self.feature_names = expression_data.index.tolist()\n",
    "\n",
    "        # Remove unknown samples\n",
    "        known_indices = [i for i, label in enumerate(self.y) if label != 'Unknown']\n",
    "        self.X = self.X.iloc[known_indices]\n",
    "        self.y = [self.y[i] for i in known_indices]\n",
    "\n",
    "        # Encode labels\n",
    "        self.y = self.label_encoder.fit_transform(self.y)\n",
    "\n",
    "        print(f\"Final data shape: {self.X.shape}\")\n",
    "        print(f\"Classes: {self.label_encoder.classes_}\")\n",
    "        print(f\"Label distribution: {np.bincount(self.y)}\")\n",
    "\n",
    "        return self.X, self.y\n",
    "\n",
    "    def feature_selection(self, n_features=2000, method='univariate'):\n",
    "        \"\"\"\n",
    "        Perform feature selection to reduce dimensionality\n",
    "\n",
    "        Args:\n",
    "            n_features (int): Number of top features to select\n",
    "            method (str): Feature selection method ('univariate', 'rfe', or 'pca')\n",
    "        \"\"\"\n",
    "        print(f\"Performing feature selection using {method} method...\")\n",
    "        original_shape = self.X.shape\n",
    "\n",
    "        if method == 'univariate':\n",
    "            # Use ANOVA F-test for feature selection\n",
    "            selector = SelectKBest(score_func=f_classif, k=min(n_features, self.X.shape[1]))\n",
    "            self.X = selector.fit_transform(self.X, self.y)\n",
    "            selected_indices = selector.get_support(indices=True)\n",
    "            self.feature_names = [self.feature_names[i] for i in selected_indices]\n",
    "\n",
    "        elif method == 'pca':\n",
    "            # Use PCA for dimensionality reduction\n",
    "            n_components = min(n_features, self.X.shape[1], self.X.shape[0]-1)\n",
    "            pca = PCA(n_components=n_components)\n",
    "            self.X = pca.fit_transform(self.X)\n",
    "            self.feature_names = [f'PC{i+1}' for i in range(self.X.shape[1])]\n",
    "            print(f\"Explained variance ratio: {sum(pca.explained_variance_ratio_):.3f}\")\n",
    "\n",
    "        elif method == 'rfe':\n",
    "            # Use Recursive Feature Elimination with Random Forest\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            n_select = min(n_features, self.X.shape[1])\n",
    "            rfe = RFE(estimator=rf, n_features_to_select=n_select)\n",
    "            self.X = rfe.fit_transform(self.X, self.y)\n",
    "            selected_indices = rfe.get_support(indices=True)\n",
    "            self.feature_names = [self.feature_names[i] for i in selected_indices]\n",
    "\n",
    "        print(f\"Feature selection completed: {original_shape} -> {self.X.shape}\")\n",
    "\n",
    "    def train_models(self):\n",
    "        \"\"\"Train multiple machine learning models\"\"\"\n",
    "        print(\"Training multiple models...\")\n",
    "\n",
    "        # Convert to numpy array if it's a DataFrame\n",
    "        if isinstance(self.X, pd.DataFrame):\n",
    "            self.X = self.X.values\n",
    "\n",
    "        # Scale features\n",
    "        self.X = self.scaler.fit_transform(self.X)\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=0.2, random_state=42, stratify=self.y\n",
    "        )\n",
    "\n",
    "        # Store test data for final evaluation\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        # Define models to train\n",
    "        models = {\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "            'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=500)\n",
    "        }\n",
    "\n",
    "        # Train and evaluate models\n",
    "        cv_scores = {}\n",
    "        self.trained_models = {}\n",
    "\n",
    "        for name, model in models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "\n",
    "            # Cross-validation\n",
    "            cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "            cv_scores[name] = cv_score\n",
    "\n",
    "            # Fit model\n",
    "            model.fit(X_train, y_train)\n",
    "            self.trained_models[name] = model\n",
    "\n",
    "            print(f\"{name} CV Accuracy: {cv_score.mean():.3f} (+/- {cv_score.std() * 2:.3f})\")\n",
    "\n",
    "        # Select best model based on CV score\n",
    "        best_model_name = max(cv_scores, key=lambda x: cv_scores[x].mean())\n",
    "        self.best_model = self.trained_models[best_model_name]\n",
    "        print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "        return cv_scores\n",
    "\n",
    "    def hyperparameter_tuning(self, model_name='Random Forest'):\n",
    "        \"\"\"Perform hyperparameter tuning for the specified model\"\"\"\n",
    "        print(f\"Performing hyperparameter tuning for {model_name}...\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=0.2, random_state=42, stratify=self.y\n",
    "        )\n",
    "\n",
    "        if model_name == 'Random Forest':\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "            model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "        elif model_name == 'SVM':\n",
    "            param_grid = {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "                'kernel': ['rbf', 'linear']\n",
    "            }\n",
    "            model = SVM(probability=True, random_state=42)\n",
    "\n",
    "        else:\n",
    "            print(f\"Hyperparameter tuning not implemented for {model_name}\")\n",
    "            return None\n",
    "\n",
    "        # Grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "        self.best_model = grid_search.best_estimator_\n",
    "        return grid_search\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate the best model on test data\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"No trained model found. Please train models first.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\nEvaluating best model on test data...\")\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = self.best_model.predict(self.X_test)\n",
    "        y_pred_proba = self.best_model.predict_proba(self.X_test)\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "        # Classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        target_names = self.label_encoder.classes_\n",
    "        print(classification_report(self.y_test, y_pred, target_names=target_names))\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=target_names, yticklabels=target_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        return y_pred, y_pred_proba\n",
    "\n",
    "    def feature_importance_analysis(self, top_n=20):\n",
    "        \"\"\"Analyze feature importance for tree-based models\"\"\"\n",
    "        if hasattr(self.best_model, 'feature_importances_'):\n",
    "            importance_scores = self.best_model.feature_importances_\n",
    "\n",
    "            # Create feature importance DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': self.feature_names,\n",
    "                'importance': importance_scores\n",
    "            }).sort_values('importance', ascending=False)\n",
    "\n",
    "            # Save top features to CSV\n",
    "            importance_df.to_csv('feature_importance.csv', index=False)\n",
    "            print(f\"Feature importance saved to feature_importance.csv\")\n",
    "\n",
    "            # Plot top features\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            top_features = importance_df.head(top_n)\n",
    "            plt.barh(range(len(top_features)), top_features['importance'][::-1])\n",
    "            plt.yticks(range(len(top_features)), top_features['feature'][::-1])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title(f'Top {top_n} Most Important Features')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "            return importance_df\n",
    "        else:\n",
    "            print(\"Feature importance not available for this model type.\")\n",
    "            return None\n",
    "\n",
    "    def save_model(self, filepath='cancer_classification_model.pkl'):\n",
    "        \"\"\"Save the trained model and preprocessing objects\"\"\"\n",
    "        model_data = {\n",
    "            'model': self.best_model,\n",
    "            'scaler': self.scaler,\n",
    "            'label_encoder': self.label_encoder,\n",
    "            'feature_names': self.feature_names\n",
    "        }\n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def predict_new_samples(self, new_data):\n",
    "        \"\"\"Predict cancer types for new samples\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"No trained model found. Please train the model first.\")\n",
    "            return None\n",
    "\n",
    "        # Scale the new data\n",
    "        new_data_scaled = self.scaler.transform(new_data)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.best_model.predict(new_data_scaled)\n",
    "        probabilities = self.best_model.predict_proba(new_data_scaled)\n",
    "\n",
    "        # Convert predictions back to original labels\n",
    "        predicted_labels = self.label_encoder.inverse_transform(predictions)\n",
    "\n",
    "        return predicted_labels, probabilities\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Initialize pipeline\n",
    "    pipeline = CancerClassificationPipeline(\n",
    "        expression_file='GSE68086_TEP_data_matrix.csv',\n",
    "        metadata_file='GSE68086_series_matrix.csv'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        X, y = pipeline.load_and_preprocess_data()\n",
    "\n",
    "        # Feature selection (you can adjust n_features and method)\n",
    "        pipeline.feature_selection(n_features=2000, method='univariate')\n",
    "\n",
    "        # Train models\n",
    "        cv_scores = pipeline.train_models()\n",
    "\n",
    "        # Hyperparameter tuning (optional - uncomment to enable)\n",
    "        # pipeline.hyperparameter_tuning('Random Forest')\n",
    "\n",
    "        # Evaluate model\n",
    "        y_pred, y_pred_proba = pipeline.evaluate_model()\n",
    "\n",
    "        # Feature importance analysis\n",
    "        importance_df = pipeline.feature_importance_analysis()\n",
    "\n",
    "        # Save model\n",
    "        pipeline.save_model()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Training completed successfully!\")\n",
    "        print(\"Files generated:\")\n",
    "        print(\"- cancer_classification_model.pkl (trained model)\")\n",
    "        print(\"- confusion_matrix.png (evaluation plot)\")\n",
    "        print(\"- feature_importance.png (importance plot)\")\n",
    "        print(\"- feature_importance.csv (importance data)\")\n",
    "\n",
    "        return pipeline, cv_scores, y_pred, y_pred_proba\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {str(e)}\")\n",
    "        print(\"Please check that the input files are in the correct format and location.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the pipeline\n",
    "    pipeline, cv_scores, y_pred, y_pred_proba = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
